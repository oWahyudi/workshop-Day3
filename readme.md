<h1>Introduction</h1>

This workshop result is submitted as part of an evaluation for Integrating and Deploying AI Solution course.

# Self-Host Workshop

## Workshop - Self-Host I

### Install and Run Ollama Serve

![alt text](image.png)

## Workshop - Self-Host II

### ollama pull gemma:2b

![alt text](image-2.png)

## Workshop - Self-Host III

### ollama run gemma:2b

![alt text](image-1.png)

## Workshop - Self-Host IV

### Set system prompt and save the model with new name

![alt text](image-3.png)

### run new custom model

![alt text](image-4.png)

![alt text](image-5.png)

## Workshop - Self-Host V

### ollama with Python

#### Install the ollama package for python

![alt text](image-6.png)

#### Execute python code

![alt text](image-7.png)

## Workshop - Self-Host VI

### Install the langchain python library

![alt text](image-9.png)

### Install and run llama2

![alt text](image-8.png)

![alt text](image-10.png)

### Ollama with python using langchain

![alt text](image-11.png)

# Testing Workshop

## Workshop – Prompt Testing I

### Install/verify/start promptfoo

![alt text](image-12.png)

![alt text](image-13.png)

![alt text](image-16.png)
![alt text](image-14.png)
![alt text](image-15.png)
![alt text](image-17.png)

## Workshop – Prompt Testing II

![alt text](image-18.png)

![alt text](image-19.png)

## Workshop – Prompt Testing III

### promptfoo eval - Test Negative Scenario, The output must less than 2000 tokens

![alt text](image-20.png)

### promptfoo eval - Test Positive Scenario, The output must less than 2500 tokens

![alt text](image-21.png)

### promptfoo view

![alt text](image-22.png)

## Workshop – Prompt Testing IV

![alt text](image-23.png)

![alt text](image-24.png)

# Token Monitoring Workshop

## Workshop – Monitoring LLM Token Usage I

### Install required packages

![alt text](image-25.png)

![alt text](image-26.png)

## Workshop – Monitoring LLM Token Usage II - III

## FastAPI with Token Metrics

![alt text](image-27.png)

![alt text](image-28.png)

## Workshop – Monitoring LLM Token Usage IV

## Run the App

![alt text](image-30.png)

## Workshop – Monitoring LLM Token Usage V

## Configuring Prometheus

![alt text](image-32.png)

## Configuring Docker Prometheus and Grafana

![alt text](image-33.png)

## Run Prometheus & Grafana in Docker

![alt text](image-31.png)

![alt text](image-34.png)

![alt text](image-35.png)

### Prometheus

![alt text](image-29.png)

![alt text](image-36.png)

### Grafana

![alt text](image-37.png)

# Langfuse Workshop

## Workshop – Langfuse I, II, III, IV, V

### Run python app

![alt text](image-38.png)

### Test Application

![alt text](image-39.png)

![alt text](image-40.png)

![alt text](image-41.png)

![alt text](image-42.png)
