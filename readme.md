<h1>Introduction</h1>

This workshop result is submitted as part of an evaluation for Integrating and Deploying AI Solution course.

# Self-Host Workshop

## Workshop - Self-Host I

### Install and Run Ollama Serve

![alt text](image.png)

## Workshop - Self-Host II

### ollama pull gemma:2b

![alt text](image-2.png)

## Workshop - Self-Host III

### ollama run gemma:2b

![alt text](image-1.png)

## Workshop - Self-Host IV

### Set system prompt and save the model with new name

![alt text](image-3.png)

### run new custom model

![alt text](image-4.png)

![alt text](image-5.png)

## Workshop - Self-Host V

### ollama with Python

#### Install the ollama package for python

![alt text](image-6.png)

#### Execute python code

![alt text](image-7.png)

## Workshop - Self-Host VI

### Install the langchain python library

![alt text](image-9.png)

### Install and run llama2

![alt text](image-8.png)

![alt text](image-10.png)

### Ollama with python using langchain

![alt text](image-11.png)

# Testing Workshop

## Workshop – Prompt Testing I

### Install/verify/start promptfoo

![alt text](image-12.png)

![alt text](image-13.png)

![alt text](image-16.png)
![alt text](image-14.png)
![alt text](image-15.png)
![alt text](image-17.png)

## Workshop – Prompt Testing II

![alt text](image-18.png)

![alt text](image-19.png)

## Workshop – Prompt Testing III

### promptfoo eval - Test Negative Scenario, The output must less than 2000 tokens

![alt text](image-20.png)

### promptfoo eval - Test Positive Scenario, The output must less than 2500 tokens

![alt text](image-21.png)

### promptfoo view

![alt text](image-22.png)

## Workshop – Prompt Testing IV

![alt text](image-23.png)

![alt text](image-24.png)

# Token Monitoring Workshop
